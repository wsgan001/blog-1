---
layout: post

title: ESL 笔记（3）

date: 2017-01-04

tags: [ESL]

comments: true

share: true

---

今天我们进入第三章的回顾，第三章是对回归模型的全面的讨论，主要关注于线性模型。这里很关键的思想就是：对**线性模型**的深入理解是进一步理解非线性模型的基础，并且很多非线性技术都是对线性方法的直接推广。
在之前的笔记中我们已经提到过线性模型可以使用最小二乘的方法进行估计，并且估计的参数可以按如下公式计算`B = (X,Y)/(X,X)`

对`Y`的估计为`X·B = X·(X,Y)/(X,X)`即`X/||X|| · (X,Y)/||X||`,所以依据最小二乘法对`Y`的估计其实就是`Y`在`X`上的正交投影

其中：`(X,Y) = ||X||·||Y||·cos(<X,Y>),(X,X) = ||X||·||X||`

换一个角度看，我们定义的模型是`Y = XB` 其实就是希望将`Y`表示在由`X`张成的超平面上，但是现实中`Y`几乎不可能完全处于超平面上，希望尽量逼近这个目标，所以最小化将`Y`投影到超平面上的距离，使用超平面上的`Y_hat`去估计`Y`

![](https://ww1.sinaimg.cn/large/006y8lVajw1fbelx1pj5oj30ow0hkt9y.jpg)
书中随后对参数B的分布进行了估计，再次此就不再展开，简单记录如下：

$$
E(\hat{B} ) = B , Var(\hat{B})=(XX^{T})^{-1}\delta^2
$$

基于参数的分布我们可以对参数的显著性进行检验

对于最小二乘估计我们有个经典结论即**高斯马尔科夫定理**：最小二乘估计在所有线性无偏估计中有着最小方差。但是值得注意的是，限制在无偏估计并不是一个必须的明智选择，有时候**有偏估计**也是不错的选择，例如之后将要讨论的岭回归（ridge regression）。考虑估计参数时的均方误差，它通常能分解为方差与偏差两部分，最小二乘能够控制没有偏差，但是在有的情况下有偏估计能够**牺牲一小部分的偏差来减少很大一部分的方差**以至于总体效果比最小二乘要好，这其实也是**方差与偏差**之间的权衡问题

现在我们开始考虑多变量回归问题，参数的估计公式能直接推广过去。可以比较容易地推出，如果各变量向量之间相互正交，即$(x_j , x_k) = 0$，对于所有`j`,`k`不等的情况,参数$b_i = (x_i , y) / (x_i , x_i)$，也就是说，当各个变量之间没有关联时，各个变量参数的估计才不会有相互影响,但是在现实中，这样的理想情况几乎不存在。

先简单考虑一个单变量带截距的回归问题，参数估计可按如下公式


$$
\beta_1 = \dfrac{(x-\bar{x}1,y)}{(x-\bar{x}1,x-\bar{x}1)}
$$

其中`1`是一个每个分量为`1`的向量

我们可以分步骤地理解这个公式

记$z = x-\bar{x}1$,那么上述公式就是计算`z`对`y`回归的参数，而`z`可以看作`x`对`1`回归后的残差

我们前面提到，回归可以理解为正交投影的过程，然后残差和投影面是正交的，所以`z`和`1`是正交的。因而刚刚的过程可以看作一个正交化的过程，我们使用两个正交的变量`1`和`z`进行回归，这样对参数的估计没有影响。

显然，以上这个过程可以进一步推广到多个变量的回归，于是得到`Regression by Successive Orthogonalization` or `Gram-Schmidt for multiple regression`算法

![](https://ww3.sinaimg.cn/large/006y8lVajw1fbeo3huxrpj31fu0l0dne.jpg)

以上算法可以得到$\{z_i\}_{i=1}^{p}$，它可以作为变量空间的一个基底，所有的$x_i$可以由该基底表示

基于如上算法我们可以得到


$$
\hat{\beta}_p=\dfrac{(z_p,y)}{(z_p,z_p)}  \ , 并且  \ \ Var(\hat{\beta}) = \dfrac{\delta^2}{(z_p,z_p)}=\dfrac{\delta^2}{\parallel z_p\parallel^2}
$$

由此可以看出，当$x_p$和某个特征$x_k$相关性比较强之时，$z_p$会接近`0`，这样参数估计的方差会十分大，参数估计显然不稳定





