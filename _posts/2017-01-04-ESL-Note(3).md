---
layout: post

title: ESL 笔记（3）

date: 2017-01-14

tags: [ESL]

comments: true

share: true

description: 

---



今天我们进入第三章的回顾，第三章是对回归模型的全面的讨论，主要关注于线性模型。这里很关键的思想就是：对**线性模型**的深入理解是进一步理解非线性模型的基础，并且很多非线性技术都是对线性方法的直接推广。
在之前的笔记中我们已经提到过线性模型可以使用最小二乘的方法进行估计，并且估计的参数可以按如下公式计算

$$
\beta= (X^{T}X)^{-1}X^{T}Y
$$


对`Y`的估计为`X·B = X·(X,Y)/(X,X)`即`X/||X|| · (X,Y)/||X||`,所以依据最小二乘法对`Y`的估计其实就是`Y`在`X`上的正交投影

其中：`(X,Y) = ||X||·||Y||·cos(<X,Y>),(X,X) = ||X||·||X||`

换一个角度看，我们定义的模型是$Y = X\beta$ 其实就是希望将`Y`表示在由`X`张成的超平面上，但是现实中`Y`几乎不可能完全处于超平面上，希望尽量逼近这个目标，所以最小化将`Y`投影到超平面上的距离，使用超平面上的`Y_hat`去估计`Y`

![](https://ww1.sinaimg.cn/large/006y8lVajw1fbelx1pj5oj30ow0hkt9y.jpg)
书中随后对参数$\beta$的分布进行了估计，再次此就不再展开，简单记录如下：

$$
E(\hat{\beta} ) = \beta , Var(\hat{\beta})=(XX^{T})^{-1}\delta^2
$$

基于参数的分布我们可以对参数的显著性进行检验

对于最小二乘估计我们有个经典结论即**高斯马尔科夫定理**：最小二乘估计在所有线性无偏估计中有着最小方差。但是值得注意的是，限制在无偏估计并不是一个必须的明智选择，有时候**有偏估计**也是不错的选择，例如之后将要讨论的岭回归（ridge regression）。考虑估计参数时的均方误差，它通常能分解为方差与偏差两部分，最小二乘能够控制没有偏差，但是在有的情况下有偏估计能够**牺牲一小部分的偏差来减少很大一部分的方差**以至于总体效果比最小二乘要好，这其实也是**方差与偏差**之间的权衡问题

现在我们开始考虑多变量回归问题，参数的估计公式能直接推广过去。可以比较容易地推出，如果各变量向量之间相互正交，即$(x_j , x_k) = 0$，对于所有`j`,`k`不等的情况,参数$b_i = (x_i , y) / (x_i , x_i)$，也就是说，当各个变量之间没有关联时，各个变量参数的估计才不会有相互影响,但是在现实中，这样的理想情况几乎不存在。

先简单考虑一个单变量带截距的回归问题，参数估计可按如下公式


$$
\beta_1 = \dfrac{(x-\bar{x}1,y)}{(x-\bar{x}1,x-\bar{x}1)}
$$

其中`1`是一个每个分量为`1`的向量

我们可以分步骤地理解这个公式

记$z = x-\bar{x}1$,那么上述公式就是计算`z`对`y`回归的参数，而`z`可以看作`x`对`1`回归后的残差

我们前面提到，回归可以理解为正交投影的过程，然后残差和投影面是正交的，所以`z`和`1`是正交的。因而刚刚的过程可以看作一个正交化的过程，我们使用两个正交的变量`1`和`z`进行回归，这样对参数的估计没有影响。

显然，以上这个过程可以进一步推广到多个变量的回归，于是得到`Regression by Successive Orthogonalization` or `Gram-Schmidt for multiple regression`算法

![](https://ww3.sinaimg.cn/large/006y8lVajw1fbeo3huxrpj31fu0l0dne.jpg)

以上算法可以得到$\{z_i\}_{i=1}^{p}$，它可以作为变量空间的一个基底，所有的$x_i$可以由该基底表示

基于如上算法我们可以得到


$$
\hat{\beta}_p=\dfrac{(z_p,y)}{(z_p,z_p)}  \ , 并且  \ \ Var(\hat{\beta}) = \dfrac{\delta^2}{(z_p,z_p)}=\dfrac{\delta^2}{\parallel z_p\parallel^2}
$$

由此可以看出，当$x_p$和某个特征$x_k$相关性比较强之时，$z_p$会接近`0`，这样参数估计的方差会十分大，参数估计显然不稳定



考虑线性模型的解释能力的时候，我们往往需要考虑变量的某一个子集，所以这就涉及到了子集选取的问题，接下来我们就讨论几个常用的子集选取办法

`Best-Subset Selection`

最佳子集选取想要完成的是从`P`个变量中选出`k`个变量，并让着`k`个变量构成的模型均方误差最小，这里`k`的选择往往需要考虑方差和偏差的权衡问题，我们通常使用的准则是：在最小化期望预测误差的模型中选择最小的那个模型

当`p`比较大的时候，对整个变量空间进行遍历式的子集选取显然是不现实的，所以我们提出了一系列逼近最佳子集选取的方式



`Forward and Backward-Stepwise Selection`

Forward的方法就是从截距开始，不断往模型中不断加入最可能提高模型效果的变量

Backward的方法是从全模型开始，依据Z-score不断的删去贡献度低的变量

有了这两个算法，很直接的想法就是是否能结合两者的特点，所以在R中有的包在变量筛选时就基于`AIC`，每一步可能添加也可能删除变量

此外我们还需要注意的一点就是某些变量是天然的捆绑在一起的，比如one-hot 变量，这些变量在选择时需要一起考虑

`Forward-Stagewise Selection`

Forward-stagewise一开始从截距 $\bar{y}$ 开始，随后每次挑选和残差最相关的变量与残差做回归，然后将这个变量和对应的系数加入模型，和Forward-Stepwise不同在于，每次加入的变量的参数不会再进行更新。该方法逼近最佳子集选取的速度比较慢，但是在高纬度的情况下，他有较强的竞争力。



子集选取模型直接剔除了那些认为不重要的变量，这可以看作是一种离散的变量选择模式，这种模式便于我们更好的解释模型并可能可以降低预测误差，但有时会引入较大的方差，这不利于降低预测误差，所以在此我们考虑Shrinkage的方法。

`Ridge Regression`

岭回归的基本想法就是在最优化目标中加入参数的二范数,从而控制参数的大小，目标函数可写成如下：

$$
\hat{\beta}^{ridge} = argmin_{\beta}\{\sum^N_{i=1}(y_i-\beta_0-\sum^{p}_{j=1}{x_{ij}\beta_j})^2+\lambda \sum^p_{j=1}{\beta_{j}^2}\} \ , \ \lambda \geq 0
$$


变量之间的相关性比较强的时候，正如我们之前说的参数估计会很不准确，比如一个参数参数很大那么另一个相关的就很小的情况，Shrink能够较好的缓解这种情况，不过在这之前我们得先对变量做归一化处理，以避免量值本身对影响程度的误判。

岭回归的参数估计可按如下公式计算

$$
\hat{\beta}^{ridge} = (X^TX+\lambda I)^{-1}X^TY
$$


在此 ,加上 $\lambda I$ 其实就是我们常用的将奇异矩阵转变为非奇艺矩阵的小技巧，其实最开始的时候，统计中的岭回归是由以上参数计算公式引入的。

对比最小二乘我们发现，当输入变量相互正交的时候，显然

$$
\hat{\beta}^{ridge} = \dfrac{\hat{\beta}}{1+\lambda}
$$


此外我们还可以从奇异值分解角度看岭回归问题

