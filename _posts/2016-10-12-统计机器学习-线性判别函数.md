---
layout: post
title: 统计机器学习： 线性判别函数
tags: [统计机器学习]
---



### 线性判别函数

线性判别函数(两类)


$$
g(x) = w^{T}x+b=
\begin{cases}
 >0 &&w_1\\
 <0 &&w_2
\end{cases}
\\
其中，g(x) =0 \ 就是分类面方程
$$


|                   直观展示                   |                    解释                    |
| :--------------------------------------: | :--------------------------------------: |
| ![](http://ww4.sinaimg.cn/large/801b780ajw1f8s30lggioj20uo0jiq4c.jpg) | 可以看出我们需要做的就是选择$$w$$, 而选择$$w$$本质上就是寻找一个最佳的投影方向. 如左图，数据投影到法向量之后就是一维数据的分类问题 |



### Fisher 线性判别

#### Fisher判别的基本思想

- 直观描述
  - 投影后的数据满足
    - 两类之间的距离尽可能远
    - 每类自身尽可能紧凑
- 数学描述
  - 考虑投影后数据的均值和方差
    - 两类数据的均值
    - 两类数据的协方差

#### 理论推导

`以两类为例子`

记原始数据的均值为$$m_1,m_2$$，协方差为$$\Sigma_1,\Sigma_2$$ , 投影后数据的均值为$$\mu_1,\mu_2$$，协方差为$$\sigma_1,\sigma_2$$

其中易得

$$
\mu_i = w^{T}m_i   ， \ 此处w 为单位向量\\
注:x^{T}y = (x,y) \ \  而内积可以看作一个向量在另一个向量上作投影，直观地看 (x,y) = \parallel x \parallel \parallel y \parallel cos(<x,y>)
\\
\sigma_{i}^{2} = E(w^{T}x-E(w^{T}x))^2 =w^{T}E(x-E(x))^2w=w^{T}\Sigma_iw
$$

Fisher准则函数：

$$
J_F(w) = \dfrac{(\mu_1-\mu_2)^2}{\sigma_1^2+\sigma_2^2}
$$

$$
w_{opt} = arg \ max  \ J_F(w)
$$

由准则函数进一步推导可得
$$
J_F(w) = \dfrac{(\mu_1-\mu_2)^2}{\sigma_1^2+\sigma_2^2} = \dfrac{(w^Tm_1-wTm_2)^2}{w^T\Sigma_1w+w^T\Sigma_2w} = \dfrac{w^T(m_1-m_2)(m_1-m_2)^Tw}{w^T(\Sigma_1+\Sigma_2)w}
$$
据此可以定义

- 类间离散度矩阵

  - $$
    S_b = (m_1-m_2)(m_1-m_2)^T
    $$

- 类内总离散度矩阵
- 
  $$
  S_w=\Sigma_1+\Sigma_2 \ \ ，S_w正定
  $$
  若$$w^TS_ww=0$$意味着所有的数据都被投影到了一点



$$J_F(w)$$有上界，所以最佳投影方向一定存在

$$
J_F(w) = \dfrac{w^TS_bw}{w^TS_ww}
\\
\Downarrow
\\
J_F(w) \leq \dfrac{\lambda(S_b)_{max}}{\lambda(S_w)_{min}}
$$

##### 最优化问题构建

一定存在最优的$$w$$使得

$$
w^TS_ww=1
$$
故我们的目标函数可以写为

$$
max \ w^TS_bw\\
s.t. \ \ w^TS_ww = 1
$$
拉格朗日乘子法转变如下

$$
L(w,\lambda) = w^TS_bw- \lambda(w^TS_ww-1)
$$
求导便得到

$$
\dfrac{\partial L(w,\lambda)}{\partial w} = S_bw - \lambda S_ww = 0
$$

> 此处因为$$S_w$$正定比可逆，如果我们等式两边同时乘$$S_{w}^{-1}$$,可得
>
> $$
> S_{w}^{-1}S_bw=\lambda w 　可见w是矩阵S_{w}^{-1}S_b的特征向量
> $$
>

继续之前公式可得

$$
(m_1-m_2)(m_1-m_2)^{T}w _{opt}= \lambda S_w w_{opt}
$$
其中$$(m_1-m_2)^Tw_{opt}$$是一个数，同时参数$$\lambda$$也是一个数，我们求解$$w$$时只关注它的方向，所以可以把两边的数字全部去掉，于是得到

$$
(m_1-m_2)=S_w w_{opt}\\
\Downarrow\\
w_{opt} = S_{w}^{-1}(m_1-m_2) = (\Sigma_1+\Sigma_2)^{-1}(m_1-m_2)
$$

参数$$b$$的确定方式就很简单了

$$
b = \dfrac{n_1\mu_2+n_2\mu_2}{n_1+n_2} , 其中n_1,n_2 是两类样本的个数
$$

