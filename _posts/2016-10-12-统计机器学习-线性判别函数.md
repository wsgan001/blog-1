---
layout: post
title: 统计机器学习： 线性判别函数
tags: [统计机器学习]
comments: true
share: true
---



### 线性判别函数

线性判别函数(两类)


$$
g(x) = w^{T}x+b=
\begin{cases}
 >0 &&w_1\\
 <0 &&w_2
\end{cases}
\\
其中，g(x) =0 \ 就是分类面方程
$$


|                   直观展示                   |                    解释                    |
| :--------------------------------------: | :--------------------------------------: |
| ![](http://ww4.sinaimg.cn/large/801b780ajw1f8s30lggioj20uo0jiq4c.jpg) | 可以看出我们需要做的就是选择$$w$$, 而选择$$w$$本质上就是寻找一个最佳的投影方向. 如左图，数据投影到法向量之后就是一维数据的分类问题 |

----

### Fisher 线性判别

#### Fisher判别的基本思想

- 直观描述
  - 投影后的数据满足
    - 两类之间的距离尽可能远
    - 每类自身尽可能紧凑
- 数学描述
  - 考虑投影后数据的均值和方差
    - 两类数据的均值
    - 两类数据的协方差

#### 理论推导

`以两类为例子`

记原始数据的均值为$$m_1,m_2$$，协方差为$$\Sigma_1,\Sigma_2$$ 

记原始数据投影后数据的均值为$$\mu_1,\mu_2$$，协方差为$$\sigma_1,\sigma_2$$



其中易得


$$
\mu_i = w^{T}m_i   ， \ 此处w 为单位向量
\\
\sigma_{i}^{2} = E(w^{T}x-E(w^{T}x))^2 =w^{T}E(x-E(x))^2w=w^{T}\Sigma_iw
$$

> 注: $$x^{T}y = (x,y)$$ , 而内积可以看作一个向量在另一个向量上作投影，直观地看 $$(x,y) = \parallel x \parallel \parallel y \parallel cos(<x,y>)$$

Fisher准则函数：


$$
J_F(w) = \dfrac{(\mu_1-\mu_2)^2}{\sigma_1^2+\sigma_2^2}
\\
w_{opt} = arg \ max  \ J_F(w)
$$



由准则函数进一步推导可得


$$
J_F(w) = \dfrac{(\mu_1-\mu_2)^2}{\sigma_1^2+\sigma_2^2} = \dfrac{(w^Tm_1-wTm_2)^2}{w^T\Sigma_1w+w^T\Sigma_2w} = \dfrac{w^T(m_1-m_2)(m_1-m_2)^Tw}{w^T(\Sigma_1+\Sigma_2)w}
$$


据此可以定义

- 类间离散度矩阵

  - $$
    S_b = (m_1-m_2)(m_1-m_2)^T
    $$

- 类内总离散度矩阵

  - $$
    S_w=\Sigma_1+\Sigma_2 \ \ ，S_w正定
    $$

  - 若$$w^TS_ww=0$$意味着所有的数据都被投影到了一点






$$J_F(w)$$有上界，所以最佳投影方向一定存在


$$
J_F(w) = \dfrac{w^TS_bw}{w^TS_ww}
\\
\Downarrow
\\
J_F(w) \leq \dfrac{\lambda(S_b)_{max}}{\lambda(S_w)_{min}}
$$



##### 最优化问题构建



一定存在最优的$$w$$使得


$$
w^TS_ww=1
$$


故我们的目标函数可以写为


$$
max \ w^TS_bw\\
s.t. \ \ w^TS_ww = 1
$$


拉格朗日乘子法转变如下

$$
L(w,\lambda) = w^TS_bw- \lambda(w^TS_ww-1)
$$


求导便得到

$$
\dfrac{\partial L(w,\lambda)}{\partial w} = S_bw - \lambda S_ww = 0
$$



> 此处因为$$S_w$$正定必可逆，如果我们等式两边同时乘$$S_{w}^{-1}$$,可得


> $$
> S_{w}^{-1}S_bw=\lambda w 　可见w是矩阵S_{w}^{-1}S_b的特征向量
> $$
>



继续之前公式可得


$$
(m_1-m_2)(m_1-m_2)^{T}w _{opt}= \lambda S_w w_{opt}
$$


其中$$(m_1-m_2)^Tw_{opt}$$是一个数，同时参数$$\lambda$$也是一个数，我们求解$$w$$时只关注它的方向，所以可以把两边的数字全部去掉，于是得到


$$
(m_1-m_2)=S_w w_{opt}\\
\Downarrow\\
w_{opt} = S_{w}^{-1}(m_1-m_2) = (\Sigma_1+\Sigma_2)^{-1}(m_1-m_2)
$$



参数$$b$$的确定方式就很简单了


$$
b = \dfrac{n_1\mu_2+n_2\mu_2}{n_1+n_2} , 其中n_1,n_2 是两类样本的个数
$$

----

### 最小平方误差准则

线性回归模型的例子


$$
f(x)=\beta_0+\sum_{i=1}^{n}x_i\beta _i
$$

将 1 放入$$X$$ 即**增广矩阵**，可写成


$$
\hat{Y} =  X\beta
$$

基于Least square 得到


$$
min  \ J(\beta )= \parallel Y-X\beta \parallel^{2}
$$

求导可得


$$
\dfrac{\partial J}{\partial \beta} = -2X^{T}(Y-X\beta_{opt}) = 0
$$

于是有


$$
Y = X \beta_{opt}
$$

避免$$X$$不可逆，两边同乘$$X^{T}$$


$$
X^{T}Y=X^{T}X\beta_{opt}
$$

所以可以得到


$$
\beta_{opt} = (X^{T}X)^{-1}X^{T}Y = \dfrac{(X,Y)}{(X,X)}
$$

----

### 最小错分样本数准则

有时，数据并不是线性可分的，比如

![](http://ww2.sinaimg.cn/large/801b780ajw1f8s81lfzffj210u0j6dhr.jpg)

这样的情况下，我们最直接的想法就是考虑最小错分量

