---
layout: post

title: ESL 笔记（2）

date: 2016-12-30

tags: [ESL]

comments: true

share: true

---

第二章由两个简单的例子引入：最小二乘(Least Squares)和最邻近算法(Nearest Neighbors)，这两个算法应该接触过一点统计学习的人都知道，但是在书中作者将两者相互联系，写得特别不错。

首先我们对两个模型的表现有一个直观的了解

- 最小二乘：基于比较强的假设，输出较为稳定，但是对数据描述有较大偏差（方差小，偏差大）
- 最邻近：输出较为精准，但是不太稳定，比如用它进行分类的时候分类边界特别不规则（方差大偏差小）


最小二乘应用于线性模型，线性模型假定输出和输入的各个变量存在线性关系。基于误差平方和建立目标函数，最小化这个函数对模型参数进行估计。我们可通过对参数求导使目标函数的一阶导数为`0`解得参数值,通常可以写为`(X,Y)/(X,X)`

最邻近的思想就是依据离观测点最近的几个对象的属性来预测这个观测点的属性，而这里的预测通常采用**平均**的方法，即对最近邻的对象的属性进行平均（例如在具体的分类中，我们取最大投票数的类别作为观测对象的类别）。此处的'最近'依据的是某一种我们定义的**距离**，最常见的是大家熟悉的欧拉距离，但是我们也可以定义其他距离来拓展模型。

在最近邻方法中很明显的一个问题就是参数`k`(最近邻的数量)的选择，并且我们并不能使用最小化误差平方和的方式来最优化挑选这个`k`值，因为使用这个方式的话，最终得到的必然是`k=1`.

随后便是比较精彩的部分，统计决策理论，在这部分里面作者便将以上两者联系起来

从**随机变量**和**概率空间**的角度考虑，假定我们的输入是随机向量`X`，输出是随机变量`Y`，对于`X`和`Y`有着一个联合概率分布`Pr(X,Y)`。我们需要找到一个函数`f`能基于给出的`X`去预测`Y`

首先考虑数值预测，基于以上想法我们建立一个损失函数（通常就是误差平方和）然后构建一个期望预测误差，将这个期望值建立在条件`X`上，最小化后我们能得到我们要求的函数f是一个条件期望，即`f(x)=E(Y|X=x)`，这个也称为回归函数

其次我们考察最近邻方法，其实我们可以很直观的发现，最近邻方法直接使用了条件期望这个想法，基于观测值，然后均值其最近邻，即`f(x)=Ave( yi | xi 是x的最近邻)` ，此处我们用均值去逼近期望。假设样本总数是`N`，`k`为最近邻的数量那么，当`N`、`k`趋向于无穷，同时`k/N`趋向于`0`的时候`f(x)`就收敛到`E(Y|X=x)`。此处存在的一个问题就是当维度提升的时候，收敛的速度会变慢，随后会提到**维度灾难**这个问题。

最近邻方法和最小二乘方法最终总结起来都可以看作是通过均值对**条件期望**的逼近，它们之间的不同在于以下两点

- 最小二乘方法假设用一个**全局**的线性函数去逼近
- 最邻近方法假设用**局部**的常值函数去逼近

接下来我们考虑输出是类别变量的时候如何定义损失函数并最小化它求解，此处我们会自然地提出**贝叶斯分类**算法。

对于分类问题我们关注的不再是数值上的误差，而是是否将对象分到了正确的类别，所以最简单的思路就是如果分对了惩罚为`0`，分错了惩罚为`1`，这样我们可以定义一个矩阵L，L的对角线都是`0`，其余位置为`1`.

基于这个想法我们便可以定义出我们的最优化目标，将对象`x`分类到类别`g`使得`x`属于每个类别的概率向量和类别`g`与所有类别计算得到的惩罚变量构成的向量（此处只有`L(g,g)=0` ,其余为`1`）的内积最小。基于这个我们可以这样理解：当我们将`x`分类到`g`时能最小化分类到其他类别的概率之和，又由于分类到所有类别的概率和为`1`，所以优化目标又可以转化为最大化将`x`分类到类别`g`的概率，即`max Pr(g|X=x) , g属于类别集合`。对`Pr(g|X=x)`的计算则可以基于贝叶斯公式`P(A|B) = P(B|A)P(A)/P(B)`

前面我们提到了最近邻方法，它给我们的印象是似乎只要训练数据足够多，我们就能够不断逼近真实的条件期望，因为基于观测对象我们能找到足够多的最近邻去平均出观测对象的属性。但是，事实是在高维空间中这个想法会失效，也称之为**维度灾难**

举个简单的例子，我们知道在`p`维空间中立方体的体积为`V = a^p`，假设我们要去观测空间中占`r`比例的空间，那么我们的期望边长就是`r^(1/p)`。比如在`10`维空间中`p=10`，我们可以计算`0.01^(1/10)=0.63` , `0.1^(1/10)=0.80` , 也就是说我们想要使用`1%`或者`10%`的数据去构建局部平均的话，我们需要涉及到数据`63%`或者`80%`的范围。

比较有意思的一个问题是去思考n维空间中的单位球，并且往无穷维拓展

高维空间还有一个问题就是大多数样本点聚集在样本空间的边界，这就导致了在边界上进行估计更加困难

还有一个问题是样本密度的问题，样本密度与`N^(1/p)`成比例（`N`为样本数，`p`为维度）。比如在一维空间中`10`个样本点构成某一样本密度，为了保持同样的样本密度，在`10`维空间中我们需要`10^(10)`这么多的样本。

可以发现最近邻方法和一些局部方法都会面临维度灾难，在实际问题中，当我们的模型在许多变量上过于复杂的时候也会面临维度灾难的问题，我们可以通过加入一些假设与限制来避免维度灾难。

接下来提一下其他几个要点

**监督学习**基本思想是对输入和输出进行学习，然后得到预测的输出，基于真实输出与预测输出我们可以计算出误差，基于这个误差我们不断去调整模型

之前我们说过模型的学习过程可以归结为函数逼近和估计，在此就关于**函数逼近**再提几点
1. 函数逼近中常常会使用基函数展开，基函数可以是幂级数、三角函数、sigmoid函数等等
2. 对参数估计的时候我们可以使用最小二乘法，但是更为常用的是使用极大似然法去估计参数，极大似然的想法很直观：当我们选取特定参数后，能使我们观测到这些观测值的概率最大

还有一点是关于是关于维度灾难的值得一提，原文如下：

> One fact should be clear by now. Any method that attempts to produce locally varying functions in small isotropic neighborhoods will run into problems in high dimensions — again the curse of dimensionality. And conversely, all methods that overcome the dimensionality problems have an associated — and often implicit or adaptive — metric for measuring neighborhoods, which basically does not allow the neighborhood to be simultaneously small in all directions.

最后以3种常用的技巧结束本章

1.添加惩罚项
惩罚项往往是对函数某一特性的约束，例如对于一维输入的cubic smoothing spline （函数二阶导数平方的积分，即二范数），当惩罚因子趋向无穷，只有线性函数会被允许。
惩罚函数，或者说**正则化**方法（regularization）表达了我们对描述问题的函数的**先验性知识**

2.核方法和局部回归
这些方式可以看作是基于局部邻居这一想法对条件期望进行估计的方法。而邻居通过**核函数**对特定点邻域中的对象赋予**权重**来表现。例如高斯核函数，他赋予点的权重依据这些点与被观测点的欧拉距离指数下降，高斯核函数中的参数控制着邻域的大小。向线性回归的损失函数中引入核函数，我们便能得到熟知的局部线性回归模型。最近邻方法也可以归类到核方法，它核函数的定义使用了一个指示函数，距离小于第`k`大的为`1`，大于的就是`0`.

3.基函数及字典方法
基函数前面已经简单提过，这里再添加几种常用的**基函数**： spline／radial／adaptive basis function ， 我们要做的就是使用合适的**搜索策略**从一大堆基函数中筛选出合适的函数。

`『ZJun 2016.12.30』`