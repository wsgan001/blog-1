---
layout: post
title: 数据仓库与数据挖掘：Lesson 4
tags: [数据仓库与数据挖掘]
comments: true
share: true
---

### 贝叶斯方法

背后的深刻原因在于，现实世界本身就是不确定的，人类的观察能力有限，看到的只是世界的表象

> 具体参见[统计机器学习： 贝叶斯决策理论](https://zjcode.github.io/blog/2016-09-28/统计机器学习-贝叶斯决策理论/)的笔记

### 神经网络

> 具体参见[机器学习：神经网络](https://zjcode.github.io/blog/2016-08-01/第五章-神经网络/)的笔记


### SVM (支撑向量机)

#### Intuition

- 如何取寻找一个最优的判别面
- 引入Margin的概念，然后最大化这个Margin , 记为M

### SVM提取最优化问题方式一

hype-Plane


$$
Plus  \ Plane :w^{T}x+b=+1\\
Minus \ Plane:w^{T}x+b=-1
$$

Maxinize Margin

Let $x^{+}$be an point on the plus-plane-point

Let $x^{-}$be an point on the minus-plane-point


$$
M = |x^{+}-x^{-}|\\
w^Tx^{+}+b=1\\
w^Tx^{-}+b=-1
$$

Then We can get


$$
x^{+}=x^{-}+\lambda w
$$

Then

$$
\lambda = \dfrac{2}{\parallel w \parallel^2}
$$

So we can get


$$
M=|x^{+}-x^{-}| = |\lambda w|=\lambda |w|=\dfrac{2}{\parallel w\parallel}
$$

In order to maximize M we can Minimize

$$
\dfrac{1}{2}\parallel w \parallel^{2}\\
s.t. \ \ w^Tx_{+}+b\geq1,w^Tx_{-}+b\leq-1
$$

### SVM提取最优化问题方式二

超平面方程

$$
w^Tx+b=0
$$

我们知道点到超平面对距离可按如下公式计算

$$
r=\dfrac{|w^Tx+b|}{||w||}
$$


假设超平面能将数据分为两类，对于$(x_i,y_i)$有

$$
\begin{cases}
w^Tx_{i}+b\geq+1 &&y_i=+1\\
w^Tx_{i}+b\leq-1 &&y_i=-1
\end{cases}
$$

![](http://ww3.sinaimg.cn/large/801b780ajw1f8u9308ptlj20w80mr3zs.jpg)

如上图，我们可以算出，两个超平面的距离为

$$
r  = \dfrac{2}{||w||}
$$

为了最大化这个距离，我们的最优化问题可定义如下

$$
min_{w,b} \ \  \dfrac{2}{||w||}\\
s.t. \ y_i(w^Tx_i+b)\geq1
$$

转换后可变为

$$
min_{w,b} \ \ \dfrac{1}{2}||w||^2\\
s.t. \ y_i(w^Tx_i+b)\geq1
$$

### SVM二次规划问题求解

问题：

$$
min_{w,b} \ \ \dfrac{1}{2}||w||^2\\
s.t. \ 1-y_i(w^Tx_i+b)\leq0
$$

首先写出其拉格朗日函数

$$
L(w,b,\alpha) = \dfrac{1}{2}||w||^2+\sum_{i=1}^{m}\alpha_i(1-y_i(w^Tx_i+b))
$$

对$w_{i},b$求偏导使其为0，得到

$$
w=\sum_{i=1}^m\alpha_iy_ix_i \\
0=\sum_{i=1}^ma_iy_i
$$

带入整理可得对偶问题：

$$
max_{\alpha} \ \  \sum_{i=1}^m\alpha_i - \dfrac{1}{2}\sum_{i=1}^m\sum_{j=1}^m\alpha_i\alpha_jy_iy_jx_i^Tx_j\\
s.t. \ \ \sum_{i=1}^m\alpha_iy_i=0\\
\alpha_i\geq0 , i=1,2,\dots m
$$

 最终可得

$$
f(x)=w^Tx+b=\sum_{i=1}^ma_iy_ix_i^Tx+b
$$

### KKT定理补充知识



> ####  Optimization problem:
>
> $$
> Minimize \ \ f(x) \\
> s.t. \ \ g_{i}(x)\leq 0,h_{j}(x)=0
> \\
> i=1,2,\dots,m  \ ;\  j=1,2,\dots l
> $$
>
> #### Stationarity
>
> $$
> For \  maximizing  \ f(x): \ \
> \nabla f(x^{*})=\sum _{i=1}^{m}\mu _{i}\nabla g_{i}(x^{*})+\sum _{j=1}^{l}\lambda _{j}\nabla h_{j}(x^{*})\\
> For \  minimizing \  f(x): \ \
> - \nabla f(x^{*})=\sum _{i=1}^{m}\mu _{i}\nabla g_{i}(x^{*})+\sum _{j=1}^{l}\lambda _{j}\nabla h_{j}(x^{*})
> $$
>
> #### Primal feasibility
>
> $$
> g_{i}(x^{*})\leq 0,{\mbox{ for all }}i=1,\ldots ,m \\
> h_{j}(x^{*})=0,{\mbox{ for all }}j=1,\ldots ,l\,\!
> $$
>
> #### Dual feasibility
>
> $$
> \mu _{i}\geq 0,{\mbox{ for all }}i=1,\ldots ,m
> $$
>
> #### Complementary slackness
>
> $$
> \mu _{i}g_{i}(x^{*})=0,{\mbox{for all}}\;i=1,\ldots ,m.
> $$
>

SVM二次规划问题求解需满足KKT，所以我们可以得到


$$
\begin{cases}
\alpha_i\geq0\\
y_if(x_i)-1\geq0\\
\alpha_i(y_if(x_i)-1)=0  \  (*)
\end{cases}
$$


由$(*)$可知,对于样本$(x_i,y_i)$,总有 $\alpha_i=0\ or \ y_if(x_i)=1$，当 $\alpha_i=0$时，该样本不会在我们最终得到的函数$f(x)=\sum_{i=1}^ma_iy_ix_i^Tx+b$ 中出现，若$\alpha_i>0$，则必有$y_if(x_i)=1$，这些点恰好在两个最大间隔边界上，这些样本点称为支撑向量


### Kernel

如果原始空间是有限维，即样本属性数有限，那么肯定存在一个高维特征空间使样本可分

常见的核函数

| 名称       | 表达式                                      | 参数                |
| -------- | ---------------------------------------- | ----------------- |
| 线性核      | $$k(x_i,x_j) = x_i^Tx_j$$                |                   |
| 多项式核     | $$k(x_i,x_j) = (x_i^Tx_j)^d$$            | $$d\geq1$$为多项式次数  |
| 高斯核      | $$k(x_i,x_j) = exp^{-\dfrac{\parallel x_i-x_j\parallel^2}{2\sigma^2}} $$ | $$\sigma$$为高斯带的带宽 |
| 拉普拉斯核    | $$k(x_i,x_j) = exp^{-\dfrac{\parallel x_i-x_j\parallel}{\sigma}}$$ | $$\sigma>0$$      |
| Sigmoid核 | $$k(x_i,x_j) = tanh(\beta x_i^Tx_j+\theta)$$ | 双曲正切函数            |



